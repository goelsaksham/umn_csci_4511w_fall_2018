<!DOCTYPE html>
<!-- saved from url=(0028)https://skymind.ai/wiki/lstm -->
<html data-wf-site="5734be9dc497153609d0cf28" data-wf-page="573b6bf09e2dc7c46dee6fc4" class=" w-mod-js w-mod-no-touch w-mod-video w-mod-no-ios gr__skymind_ai" style=""><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<title>
    
      A Beginner's Guide to LSTMs and Recurrent Neural Networks | Skymind
    
  </title>
<meta content="Skymind" property="og:site_name">
<meta content="A Beginner&#39;s Guide to LSTMs and Recurrent Neural Networks" property="og:title">
<meta content="article" property="og:type">
<meta content="LSTMs are a powerful kind of RNN used for processing sequential data such as sound, time series (sensor) data or written natural language." property="og:description">
<meta name="description" content="LSTMs are a powerful kind of RNN used for processing sequential data such as sound, time series (sensor) data or written natural language.">
<meta content="http://skymind.ai/wiki/lstm" property="og:url">
<meta name="twitter:card" content="summary">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="shortcut icon" type="image/x-icon" href="https://skymind.ai/images/favicon.png">
<link rel="stylesheet" type="text/css" href="./A Beginner&#39;s Guide to LSTMs and Recurrent Neural Networks _ Skymind_files/normalize.css">
<link rel="stylesheet" type="text/css" href="./A Beginner&#39;s Guide to LSTMs and Recurrent Neural Networks _ Skymind_files/components.css">
<link rel="stylesheet" type="text/css" href="./A Beginner&#39;s Guide to LSTMs and Recurrent Neural Networks _ Skymind_files/skymind.css">
<link rel="stylesheet" href="./A Beginner&#39;s Guide to LSTMs and Recurrent Neural Networks _ Skymind_files/font-awesome.min.css">

<style>.async-hide { opacity: 0 !important} </style>
<script type="text/javascript" src="./A Beginner&#39;s Guide to LSTMs and Recurrent Neural Networks _ Skymind_files/pd.js.download"></script><script src="./A Beginner&#39;s Guide to LSTMs and Recurrent Neural Networks _ Skymind_files/conversations-embed.js.download" type="text/javascript" id="hubspot-messages-loader" data-loader="hs-scriptloader" data-hsjs-portal="2179705" data-hsjs-env="prod"></script><script src="./A Beginner&#39;s Guide to LSTMs and Recurrent Neural Networks _ Skymind_files/2179705.js.download" type="text/javascript" id="hs-analytics"></script><script type="text/javascript" async="" src="./A Beginner&#39;s Guide to LSTMs and Recurrent Neural Networks _ Skymind_files/5605.js.download"></script><script type="text/javascript" async="" src="./A Beginner&#39;s Guide to LSTMs and Recurrent Neural Networks _ Skymind_files/analytics.js.download"></script><script type="text/javascript" async="" src="./A Beginner&#39;s Guide to LSTMs and Recurrent Neural Networks _ Skymind_files/js"></script><script async="" src="./A Beginner&#39;s Guide to LSTMs and Recurrent Neural Networks _ Skymind_files/gtm.js(1).download"></script><script async="" src="./A Beginner&#39;s Guide to LSTMs and Recurrent Neural Networks _ Skymind_files/analytics.js.download"></script><script>(function(a,s,y,n,c,h,i,d,e){s.className+=' '+y;h.start=1*new Date;
  h.end=i=function(){s.className=s.className.replace(RegExp(' ?'+y),'')};
  (a[n]=a[n]||[]).hide=h;setTimeout(function(){i();h.end=null},c);h.timeout=c;
  })(window,document.documentElement,'async-hide','dataLayer',4000,
  {'GTM-T2DSBKT':true});</script>
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-48811288-3', 'auto');
    ga('require', 'GTM-T2DSBKT');
    ga('send', 'pageview');
  </script>

<script type="text/javascript" src="./A Beginner&#39;s Guide to LSTMs and Recurrent Neural Networks _ Skymind_files/modernizr.js.download"></script>
<link rel="apple-touch-icon" href="https://skymind.ai/images/webclip.png">
<script src="./A Beginner&#39;s Guide to LSTMs and Recurrent Neural Networks _ Skymind_files/jquery-3.2.1.js.download" integrity="sha256-DZAnKJ/6XZ9si04Hgrsxu/8s717jcIzLy3oi35EouyE=" crossorigin="anonymous"></script>

<noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-P824SK"
  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  '//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-P824SK');</script>

<script type="text/javascript" async="" src="./A Beginner&#39;s Guide to LSTMs and Recurrent Neural Networks _ Skymind_files/embed-controller.js.download" charset="utf-8"></script><style type="text/css">.imgur-embed-iframe-pub { box-shadow: 0px 0px 5px 0px rgba(0, 0, 0, 0.10); border: 1px solid #ddd; border-radius: 2px;} blockquote.imgur-embed-pub { width: 540px; }</style><style type="text/css">#imgur-embed-iframe-pub-kpZBDfV { height: 562px !important;width:540px !important;}</style><script type="text/javascript" src="./A Beginner&#39;s Guide to LSTMs and Recurrent Neural Networks _ Skymind_files/analytics(2)"></script></head>
<body data-gr-c-s-loaded="true"><style type="text/css">html.hs-messages-widget-open.hs-messages-mobile,html.hs-messages-widget-open.hs-messages-mobile body{height:100%!important;overflow:hidden!important;position:relative!important}html.hs-messages-widget-open.hs-messages-mobile body{margin:0!important}#hubspot-messages-iframe-container{display:initial!important;z-index:2147483647;position:fixed!important;bottom:0!important;right:0!important}#hubspot-messages-iframe-container.internal{z-index:1016}#hubspot-messages-iframe-container.internal iframe{min-width:108px}#hubspot-messages-iframe-container .shadow{display:initial!important;z-index:-1;position:absolute;width:0;height:0;bottom:0;right:0;content:""}#hubspot-messages-iframe-container .shadow.internal{display:none!important}#hubspot-messages-iframe-container .shadow.active{width:400px;height:400px;background:radial-gradient(ellipse at bottom right,rgba(29,39,54,.16) 0,rgba(29,39,54,0) 72%)}#hubspot-messages-iframe-container iframe{display:initial!important;width:100%!important;height:100%!important;border:none!important;position:absolute!important;bottom:0!important;right:0!important;background:transparent!important}</style>

<div class="w-section hero-wiki">
<div data-collapse="small" data-animation="default" data-duration="400" data-contain="1" class="w-nav navbar-container">
<div class="w-container navbar-container"><a href="https://skymind.ai/contact" class="w-button navbar-contact--cta">CONTACT US</a>
<a href="https://skymind.ai/" class="w-nav-brand navbar-logo"><img src="./A Beginner&#39;s Guide to LSTMs and Recurrent Neural Networks _ Skymind_files/logo_skymind_white.svg" class="img-logo">
</a>
<nav class="w-nav-menu navbar-menu" role="navigation">

<a class="w-nav-link navbar-link" href="https://skymind.ai/platform" style="max-width: 940px;">AI Platform</a>

<div class="w-dropdown" data-delay="0" style="max-width: 940px;">
<div class="w-dropdown-toggle navbar-dropdown">
<div class="navbar-link">Why Skymind?</div>
</div>
<nav class="w-dropdown-list">
<a class="w-dropdown-link" href="https://skymind.ai/audience/datascientist">Data Scientists</a>
<a class="w-dropdown-link" href="https://skymind.ai/audience/architect">Solution Architects</a>
<a class="w-dropdown-link" href="https://skymind.ai/audience/devops">DevOps and SRE</a>
<a class="w-dropdown-link" href="https://skymind.ai/audience/executives">Innovation Leaders</a>
</nav>
</div>

<a class="w-nav-link navbar-link" href="https://skymind.ai/solutions" style="max-width: 940px;">Solutions</a>

<div class="w-dropdown" data-delay="0" style="max-width: 940px;">
<div class="w-dropdown-toggle navbar-dropdown">
<div class="navbar-link">Case Studies</div>
</div>
<nav class="w-dropdown-list">
<a class="w-dropdown-link" href="https://skymind.ai/case-studies/rpa">RPA</a>
<a class="w-dropdown-link" href="https://skymind.ai/case-studies/orange">Telecom Fraud</a>
<a class="w-dropdown-link" href="https://skymind.ai/case-studies/insurance">Insurance</a>
<a class="w-dropdown-link" href="https://skymind.ai/case-studies/logistics">Supply Chain &amp; Logistics</a>
<a class="w-dropdown-link" href="https://skymind.ai/case-studies/canonical">Cybersecurity and Data Centers</a>
<a class="w-dropdown-link" href="https://skymind.ai/case-studies/finance">Financial Services</a>
<a class="w-dropdown-link" href="https://skymind.ai/case-studies/image">Image Recognition</a>
<a class="w-dropdown-link" href="https://skymind.ai/case-studies/commerce">Commerce and CRM</a>
</nav>
</div>

<a class="w-nav-link navbar-link" href="https://www.skymind.ai/about" style="max-width: 940px;">About</a>

<div class="w-dropdown" data-delay="0" style="max-width: 940px;">
<div class="w-dropdown-toggle navbar-dropdown">
<div class="navbar-link">Resources</div>
</div>
<nav class="w-dropdown-list">
<a class="w-dropdown-link" href="https://docs.skymind.ai/docs" target="_blank">Documentation</a>
<a class="w-dropdown-link" href="https://github.com/SkymindIO/SKIL-CE/issues" target="_blank">Community Support</a>
<a class="w-dropdown-link" href="https://blog.skymind.ai/" target="_blank">Blog</a>
<a class="w-dropdown-link" href="https://skymind.ai/wiki/">AI Wiki</a>
<a class="w-dropdown-link" href="https://skymind.ai/open-source.html">Open-Source</a>
</nav>
</div>
</nav>
<div class="w-nav-button menu-hamburger">
<div class="w-icon-nav-menu icon-hamburger"></div>
</div>
</div>
<div class="w-nav-overlay" data-wf-ignore=""></div></div>
<div class="w-container">
<div class="hero-sub-div">
<h1 class="hero-h1-small">A.I. Wiki</h1>
</div>
</div>
</div>
<div class="w-section section-cta">
<div class="w-container container-newsletter">
<div class="w-row">
<div class="w-col w-col-7" style="margin-top: 22px">
<p class="body-h3-cta">Do you like this content? We'll send you more.</p>
</div>
<div class="w-col w-col-5" style="margin-top: 13px">
<div class="w-form">
<iframe src="./A Beginner&#39;s Guide to LSTMs and Recurrent Neural Networks _ Skymind_files/jsv68v.html" width="100%" height="40px" type="text/html" frameborder="0" allowtransparency="true" style="border: 0"></iframe>
</div>
</div>
</div>
</div>
</div>
<div class="w-section section-white">
<div class="container-white w-wiki-container">
<div class="w-row">
<div class="w-col w-col-4 wiki-nav">

<input type="checkbox" id="wiki-toggle">
<label for="wiki-toggle">
<span class="w-icon-nav-menu icon-hamburger"></span>
Directory
</label>

<div class="collapseable">
<div class="wiki-home">
<a href="https://skymind.ai/wiki/">Artificial Intelligence Wiki</a>
</div>
<div class="wiki-search">
<input id="wiki-search-input" class="wiki-search-input" type="text" placeholder="Search articles..." onkeyup="wikiSearchFn()">
</div>
<div class="wiki-links">
<ul id="wiki-links-ul">
<li><a href="https://skymind.ai/wiki/accuracy-precision-recall-f1"></a></li>
<li><a href="https://skymind.ai/wiki/active-learning">Active Learning</a></li>
<li><a href="https://skymind.ai/wiki/ai-infrastructure-machine-learning-operations-mlops"></a></li>
<li><a href="https://skymind.ai/wiki/ai-vs-machine-learning-vs-deep-learning">AI vs. ML vs. DL</a></li>
<li><a href="https://skymind.ai/wiki/ai-winter"></a></li>
<li><a href="https://skymind.ai/wiki/apache-spark-deep-learning">Apache Spark</a></li>
<li><a href="https://skymind.ai/wiki/arbiter">Arbiter</a></li>
<li><a href="https://skymind.ai/wiki/artificial-intelligence-ai">Artificial Intelligence (AI)</a></li>
<li><a href="https://skymind.ai/wiki/attention-mechanism-memory-network">Attention Mechanism Memory Networks</a></li>
<li><a href="https://skymind.ai/wiki/automl-automated-machine-learning-ai">Automated Machine Learning &amp; AI</a></li>
<li><a href="https://skymind.ai/wiki/autonomous-vehicle">Autonomous Vehicle</a></li>
<li><a href="https://skymind.ai/wiki/backpropagation">Backpropagation</a></li>
<li><a href="https://skymind.ai/wiki/bagofwords-tf-idf">Bag of Words &amp; TF-IDF</a></li>
<li><a href="https://skymind.ai/wiki/comparison-frameworks-dl4j-tensorflow-pytorch"></a></li>
<li><a href="https://skymind.ai/wiki/convolutional-network">Convolutional Neural Network (CNN)</a></li>
<li><a href="https://skymind.ai/wiki/data-for-deep-learning">Data for Deep Learning</a></li>
<li><a href="https://skymind.ai/wiki/datasets-ml">Datasets and Machine Learning</a></li>
<li><a href="https://skymind.ai/wiki/datavec"></a></li>
<li><a href="https://skymind.ai/wiki/decision-tree">Decision Tree</a></li>
<li><a href="https://skymind.ai/wiki/deep-autoencoder">Deep Autoencoders</a></li>
<li><a href="https://skymind.ai/wiki/deep-belief-network">Deep-Belief Networks</a></li>
<li><a href="https://skymind.ai/wiki/deep-reinforcement-learning">Deep Reinforcement Learning</a></li>
<li><a href="https://skymind.ai/wiki/deeplearning-research-papers">Deep Learning Resources</a></li>
<li><a href="https://skymind.ai/wiki/deeplearning4j">Deeplearning4j</a></li>
<li><a href="https://skymind.ai/wiki/denoising-autoencoder">Denoising Autoencoders</a></li>
<li><a href="https://skymind.ai/wiki/devops-machine-learning">Machine Learning DevOps</a></li>
<li><a href="https://skymind.ai/wiki/differentiableprogramming">Differentiable Programming</a></li>
<li><a href="https://skymind.ai/wiki/eigenvector">Eigenvectors, Eigenvalues, PCA, Covariance and Entropy</a></li>
<li><a href="https://skymind.ai/wiki/evolutionary-genetic-algorithm">Evolutionary &amp; Genetic Algorithms</a></li>
 <li><a href="https://skymind.ai/wiki/fraud-detection">Fraud and Anomaly Detection</a></li>
<li><a href="https://skymind.ai/wiki/generative-adversarial-network-gan">Generative Adversarial Network (GAN)</a></li>
<li><a href="https://skymind.ai/wiki/glossary">Glossary</a></li>
<li><a href="https://skymind.ai/wiki/gluon">Gluon</a></li>
<li><a href="https://skymind.ai/wiki/graph-analysis">Graph Analytics</a></li>
<li><a href="https://skymind.ai/wiki/hopfieldnetworks">Hopfield Networks</a></li>
<li><a href="https://skymind.ai/wiki/hyperparameter">Hyperparameter</a></li>
<li><a href="https://skymind.ai/wiki/index">Wiki Home</a></li>
<li><a href="https://skymind.ai/wiki/java-ai">Java AI</a></li>
<li><a href="https://skymind.ai/wiki/jumpy">Jumpy</a></li>
<li><a href="https://skymind.ai/wiki/logistic-regression">Logistic Regression</a></li>
<li><a href="https://skymind.ai/wiki/lstm" class="w--current">LSTM</a></li>
<li><a href="https://skymind.ai/wiki/machine-learning-algorithms">Machine Learning Algorithms</a></li>
<li><a href="https://skymind.ai/wiki/machine-learning-demos">Machine Learning Demos</a></li>
<li><a href="https://skymind.ai/wiki/machine-learning-library-software">Machine Learning Software</a></li>
<li><a href="https://skymind.ai/wiki/machine-learning-operations-mlops">Machine Learning Operations (MLOps)</a></li>
<li><a href="https://skymind.ai/wiki/machine-learning-research-groups-labs">Machine Learning Research Groups &amp; Labs</a></li>
<li><a href="https://skymind.ai/wiki/machine-learning-workflow">Machine Learning Workflows</a></li>
<li><a href="https://skymind.ai/wiki/machine-learning">Machine Learning</a></li>
<li><a href="https://skymind.ai/wiki/markov-chain-monte-carlo">Markov Chain Monte Carlo</a></li>
<li><a href="https://skymind.ai/wiki/multilayer-perceptron">Multilayer Perceptron</a></li>
<li><a href="https://skymind.ai/wiki/natural-language-processing-nlp">Natural Language Processing (NLP)</a></li>
<li><a href="https://skymind.ai/wiki/nd4j">ND4J</a></li>
<li><a href="https://skymind.ai/wiki/neural-network-tuning">Neural Network Tuning</a></li>
<li><a href="https://skymind.ai/wiki/neural-network">Neural Network</a></li>
<li><a href="https://skymind.ai/wiki/open-datasets">Open Datasets</a></li>
<li><a href="https://skymind.ai/wiki/radial-basis-function-network-rbf">Radial Basis Function Networks</a></li>
<li><a href="https://skymind.ai/wiki/random-forest">Random Forest</a></li>
<li><a href="https://skymind.ai/wiki/recurrent-network-rnn">Recurrent Network (RNN)</a></li>
<li><a href="https://skymind.ai/wiki/recursive-neural-tensor-network">Recursive Neural Tensor Network</a></li>
<li><a href="https://skymind.ai/wiki/restricted-boltzmann-machine">Restricted Boltzmann Machine (RBM)</a></li>
<li><a href="https://skymind.ai/wiki/robotic-process-automation-rpa">Robotic Process Automation (RPA)</a></li>
<li><a href="https://skymind.ai/wiki/scala-ai">Scala AI</a></li>
<li><a href="https://skymind.ai/wiki/single-layer-network">Single-layer Network</a></li>
<li><a href="https://skymind.ai/wiki/skynet">Skynet</a></li>
<li><a href="https://skymind.ai/wiki/spiking-neural-network-snn">Spiking Neural Networks</a></li>
<li><a href="https://skymind.ai/wiki/stacked-denoising-autoencoder">Stacked Denoising Autoencoder (SDA)</a></li>
<li><a href="https://skymind.ai/wiki/strong-ai-general-ai">Strong AI &amp; General AI</a></li>
<li><a href="https://skymind.ai/wiki/supervised-learning">Supervised Learning</a></li>

<li><a href="https://skymind.ai/wiki/symbolic-reasoning">Symbolic Reasoning</a></li>
<li><a href="https://skymind.ai/wiki/text-analysis">Text Analysis</a></li>
<li><a href="https://skymind.ai/wiki/thought-vectors">Thought Vectors</a></li>
<li><a href="https://skymind.ai/wiki/unsupervised-learning">Unsupervised Learning</a></li>
<li><a href="https://skymind.ai/wiki/use-cases">Deep Learning Use Cases</a></li>
<li><a href="https://skymind.ai/wiki/variational-autoencoder">Variational Autoencoder (VAE)</a></li>
<li><a href="https://skymind.ai/wiki/word2vec">Word2Vec, Doc2Vec and Neural Word Embeddings</a></li>

</ul>
</div>
</div>
</div>
<div class="w-col w-col-8 wiki-content">
<img width="42" src="./A Beginner&#39;s Guide to LSTMs and Recurrent Neural Networks _ Skymind_files/hr_white.png" class="hr-left">
<h1 class="wiki-title">A Beginner's Guide to LSTMs and Recurrent Neural Networks</h1>
<blockquote>
<p>Data can only be understood backwards; but it must be lived forwards. — Søren Kierkegaard, Journals</p>
</blockquote>
<p>Contents</p>
<ul>
<li><a href="https://skymind.ai/wiki/lstm#feedforward">Feedforward Networks</a></li>
<li><a href="https://skymind.ai/wiki/lstm#recurrent">Recurrent Networks</a></li>
<li><a href="https://skymind.ai/wiki/lstm#backpropagation">Backpropagation Through Time</a></li>
<li><a href="https://skymind.ai/wiki/lstm#vanishing">Vanishing and Exploding Gradients</a></li>
<li><a href="https://skymind.ai/wiki/lstm#long">Long Short-Term Memory Units (LSTMs)</a></li>
<li><a href="https://skymind.ai/wiki/lstm#capturing">Capturing Diverse Time Scales</a></li>
<li><a href="https://skymind.ai/wiki/lstm#code">Code Sample &amp; Comments</a></li>
<li><a href="https://skymind.ai/wiki/lstm#resources">Resources</a></li>
</ul>
<p>Actually, Søren Kierkegaard didn’t say that: instead of “data”, he used the word “life”. But for an algorithm, the two words are interchangeable, and it’s the algorithm’s understanding that we care about.</p>
<p>The purpose of this post is to give students of neural networks an intuition about the functioning of recurrent neural networks and purpose and structure of a prominent RNN variation, LSTMs.</p>
<p>Recurrent nets are a type of artificial neural network designed to recognize patterns in sequences of data, such as text, genomes, handwriting, the spoken word, or numerical times series data emanating from sensors, stock markets and government agencies. These algorithms take time and sequence into account, they have a temporal dimension.</p>
<p>Research shows them to be one of the most powerful and useful type of neural network, alongside the <a href="https://skymind.ai/wiki/attention-mechanism-memory-network">attention mechanism and memory networks</a>. RNNs are applicable even to images, which can be decomposed into a series of patches and treated as a sequence.</p>
<p>Since recurrent networks possess a certain type of memory, and memory is also part of the human condition, we’ll make repeated analogies to memory in the brain.<sup><a href="https://skymind.ai/wiki/lstm#one">1</a></sup></p>
<p><a class="w-button button-skilcta" href="https://nbviewer.jupyter.org/github/SkymindIO/skil-python/blob/master/examples/keras-skil-example.ipynb" style="width:75%; margin-top: 15px;" target="_blank">Getting Started with SKIL from Python</a></p>
<h2 id="review-of-feedforward-networks"><a name="feedforward">Review of Feedforward Networks</a></h2>
<p>To understand recurrent nets, first you have to understand the basics of <a href="https://skymind.ai/wiki/restricted-boltzmann-machine">feedforward nets</a>. Both of these networks are named after the way they channel information through a series of mathematical operations performed at the nodes of the network. One feeds information straight through (never touching a given node twice), while the other cycles it through a loop, and the latter are called recurrent.</p>
<p>In the case of feedforward networks, input examples are fed to the network and transformed into an output; with supervised learning, the output would be a label, a name applied to the input. That is, they map raw data to categories, recognizing patterns that may signal, for example, that an input image should be labeled “cat” or “elephant.”</p>
<p><img src="./A Beginner&#39;s Guide to LSTMs and Recurrent Neural Networks _ Skymind_files/feedforward_rumelhart.png" alt="Alt text"></p>
<p>A feedforward network is trained on labeled images until it minimizes the error it makes when guessing their categories. With the trained set of parameters (or weights, collectively known as a model), the network sallies forth to categorize data it has never seen. A trained feedforward network can be exposed to any random collection of photographs, and the first photograph it is exposed to will not necessarily alter how it classifies the second. Seeing photograph of a cat will not lead the net to perceive an elephant next.</p>
<p>That is, a feedforward network has no notion of order in time, and the only input it considers is the current example it has been exposed to. Feedforward networks are amnesiacs regarding their recent past; they remember nostalgically only the formative moments of training.</p>
<h2 id="recurrent-networks"><a name="recurrent">Recurrent Networks</a></h2>
<p>Recurrent networks, on the other hand, take as their input not just the current input example they see, but also what they have perceived previously in time. Here’s a diagram of an early, <a href="https://web.stanford.edu/group/pdplab/pdphandbook/handbookch8.html">simple recurrent net proposed by Elman</a>, where the <em>BTSXPE</em> at the bottom of the drawing represents the input example in the current moment, and <em>CONTEXT UNIT</em> represents the output of the previous moment.</p>
<p><img src="./A Beginner&#39;s Guide to LSTMs and Recurrent Neural Networks _ Skymind_files/srn_elman.png" alt="Alt text"></p>
<p>The decision a recurrent net reached at time step <code class="highlighter-rouge">t-1</code> affects the decision it will reach one moment later at time step <code class="highlighter-rouge">t</code>. So recurrent networks have two sources of input, the present and the recent past, which combine to determine how they respond to new data, much as we do in life.</p>
<p>Recurrent networks are distinguished from feedforward networks by that feedback loop connected to their past decisions, ingesting their own outputs moment after moment as input. It is often said that recurrent networks have memory.<sup><a href="https://skymind.ai/wiki/lstm#two">2</a></sup> Adding memory to neural networks has a purpose: There is information in the sequence itself, and recurrent nets use it to perform tasks that feedforward networks can’t.</p>
<p>That sequential information is preserved in the recurrent network’s hidden state, which manages to span many time steps as it cascades forward to affect the processing of each new example. It is finding correlations between events separated by many moments, and these correlations are called “long-term dependencies”, because an event downstream in time depends upon, and is a function of, one or more events that came before. One way to think about RNNs is this: they are a way to share weights over time.</p>
<p>Just as human memory circulates invisibly within a body, affecting our behavior without revealing its full shape, information circulates in the hidden states of recurrent nets. The English language is full of words that describe the feedback loops of memory. When we say a person is haunted by their deeds, for example, we are simply talking about the consequences that past outputs wreak on present time. The French call this “<em>Le passé qui ne passe pas</em>,” or “The past that does not pass away.”</p>
<p>We’ll describe the process of carrying memory forward mathematically:</p>
<p><img src="./A Beginner&#39;s Guide to LSTMs and Recurrent Neural Networks _ Skymind_files/recurrent_equation.png" alt="Alt text"></p>
<p>The hidden state at time step t is <code class="highlighter-rouge">h_t</code>. It is a function of the input at the same time step <code class="highlighter-rouge">x_t</code>, modified by a weight matrix <code class="highlighter-rouge">W</code> (like the one we used for feedforward nets) added to the hidden state of the previous time step <code class="highlighter-rouge">h_t-1</code> multiplied by its own hidden-state-to-hidden-state matrix <code class="highlighter-rouge">U</code>, otherwise known as a transition matrix and similar to a Markov chain. The weight matrices are filters that determine how much importance to accord to both the present input and the past hidden state. The error they generate will return via backpropagation and be used to adjust their weights until error can’t go any lower.</p>
<p>The sum of the weight input and hidden state is squashed by the function <code class="highlighter-rouge">φ</code> – either a logistic sigmoid function or tanh, depending – which is a standard tool for condensing very large or very small values into a logistic space, as well as making <a href="https://skymind.ai/wiki/glossary#gradient">gradients</a> workable for backpropagation.</p>
<p>Because this feedback loop occurs at every time step in the series, each hidden state contains traces not only of the previous hidden state, but also of all those that preceded <code class="highlighter-rouge">h_t-1</code> for as long as memory can persist.</p>
<p>Given a series of letters, a recurrent network <em>will</em> use the first character to help determine its perception of the second character, such that an initial <code class="highlighter-rouge">q</code> might lead it to infer that the next letter will be <code class="highlighter-rouge">u</code>, while an initial <code class="highlighter-rouge">t</code> might lead it to infer that the next letter will be <code class="highlighter-rouge">h</code>.</p>
<p>Since recurrent nets span time, they are probably best illustrated with animation (the first vertical line of nodes to appear can be thought of as a feedforward network, which becomes recurrent as it unfurls over time).</p>
<iframe allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true" class="imgur-embed-iframe-pub imgur-embed-iframe-pub-kpZBDfV-true-540" scrolling="no" src="./A Beginner&#39;s Guide to LSTMs and Recurrent Neural Networks _ Skymind_files/embed.html" id="imgur-embed-iframe-pub-kpZBDfV" style="height: 500px; width: 540px; margin: 10px 0px; padding: 0px;"></iframe><script async="" src="./A Beginner&#39;s Guide to LSTMs and Recurrent Neural Networks _ Skymind_files/embed.js.download" charset="utf-8"></script>
<p>In the <a href="http://imgur.com/kpZBDfV">diagram above</a>, each <code class="highlighter-rouge">x</code> is an input example, <code class="highlighter-rouge">w</code> is the weights that filter inputs, <code class="highlighter-rouge">a</code> is the activation of the hidden layer (a combination of weighted input and the previous hidden state), and <code class="highlighter-rouge">b</code> is the output of the hidden layer after it has been transformed, or squashed, using a rectified linear or sigmoid unit.</p>
<h2 id="backpropagation-through-time-bptt"><a name="backpropagation">Backpropagation Through Time (BPTT)</a></h2>
<p>Remember, the purpose of recurrent nets is to accurately classify sequential input. We rely on the backpropagation of error and gradient descent to do so.</p>
<p>Backpropagation in feedforward networks moves backward from the final error through the outputs, weights and inputs of each hidden layer, assigning those weights responsibility for a portion of the error by calculating their partial derivatives – <em>∂E/∂w</em>, or the relationship between their rates of change. Those derivatives are then used by our learning rule, gradient descent, to adjust the weights up or down, whichever direction decreases error.</p>
<p>Recurrent networks rely on an extension of backpropagation called <a href="https://www.cs.cmu.edu/~bhiksha/courses/deeplearning/Fall.2015/pdfs/Werbos.backprop.pdf">backpropagation through time</a>, or BPTT. Time, in this case, is simply expressed by a well-defined, ordered series of calculations linking one time step to the next, which is all backpropagation needs to work.</p>
<p>Neural networks, whether they are recurrent or not, are simply nested composite functions like <code class="highlighter-rouge">f(g(h(x)))</code>. Adding a time element only extends the series of functions for which we calculate derivatives with the chain rule.</p>
<h3 id="truncated-bptt">Truncated BPTT</h3>
<p><a href="http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf">Truncated BPTT</a> is an approximation of full BPTT that is preferred for long sequences, since full BPTT’s forward/backward cost per parameter update becomes very high over many time steps. The downside is that the gradient can only flow back so far due to that truncation, so the network can’t learn dependencies that are as long as in full BPTT.</p>
<h2 id="vanishing-and-exploding-gradients"><a name="vanishing">Vanishing (and Exploding) Gradients</a></h2>
<p>Like most neural networks, recurrent nets are old. By the early 1990s, the <em>vanishing gradient problem</em> emerged as a major obstacle to recurrent net performance.</p>
<p>Just as a straight line expresses a change in x alongside a change in y, the <em>gradient</em> expresses the change in all weights with regard to the change in error. If we can’t know the gradient, we can’t adjust the weights in a direction that will decrease error, and our network ceases to learn.</p>
<p>Recurrent nets seeking to establish connections between a final output and events many time steps before were hobbled, because it is very difficult to know how much importance to accord to remote inputs. (Like great-great-*-grandparents, they multiply quickly in number and their legacy is often obscure.)</p>
<p>This is partially because the information flowing through neural nets passes through many stages of multiplication.</p>
<p>Everyone who has studied compound interest knows that any quantity multiplied frequently by an amount slightly greater than one can become immeasurably large (indeed, that simple mathematical truth underpins network effects and inevitable social inequalities). But its inverse, multiplying by a quantity less than one, is also true. Gamblers go bankrupt fast when they win just 97 cents on every dollar they put in the slots.</p>
<p>Because the layers and time steps of deep neural networks relate to each other through multiplication, derivatives are susceptible to vanishing or exploding.</p>
<p>Exploding gradients treat every weight as though it were the proverbial butterfly whose flapping wings cause a distant hurricane. Those weights’ gradients become saturated on the high end; i.e. they are presumed to be too powerful. But exploding gradients can be solved relatively easily, because they can be truncated or squashed. Vanishing gradients can become too small for computers to work with or for networks to learn – a harder problem to solve.</p>
<p>Below you see the effects of applying a sigmoid function over and over again. The data is flattened until, for large stretches, it has no detectable slope. This is analogous to a gradient vanishing as it passes through many layers.</p>
<p><img src="./A Beginner&#39;s Guide to LSTMs and Recurrent Neural Networks _ Skymind_files/sigmoid_vanishing_gradient.png" alt="Alt text"></p>
<h2 id="long-short-term-memory-units-lstms"><a name="long">Long Short-Term Memory Units (LSTMs)</a></h2>
<p>In the mid-90s, a variation of recurrent net with so-called Long Short-Term Memory units, or LSTMs, was proposed by the German researchers Sepp Hochreiter and Juergen Schmidhuber as a solution to the vanishing gradient problem.</p>
<p>LSTMs help preserve the error that can be backpropagated through time and layers. By maintaining a more constant error, they allow recurrent nets to continue to learn over many time steps (over 1000), thereby opening a channel to link causes and effects remotely. This is one of the central challenges to machine learning and AI, since algorithms are frequently confronted by environments where reward signals are sparse and delayed, such as life itself. (Religious thinkers have tackled this same problem with ideas of karma or divine reward, theorizing invisible and distant consequences to our actions.)</p>
<p>LSTMs contain information outside the normal flow of the recurrent network in a gated cell. Information can be stored in, written to, or read from a cell, much like data in a computer’s memory. The cell makes decisions about what to store, and when to allow reads, writes and erasures, via gates that open and close. Unlike the digital storage on computers, however, these gates are analog, implemented with element-wise multiplication by sigmoids, which are all in the range of 0-1. Analog has the advantage over digital of being differentiable, and therefore suitable for backpropagation.</p>
<p>Those gates act on the signals they receive, and similar to the neural network’s nodes, they block or pass on information based on its strength and import, which they filter with their own sets of weights. Those weights, like the weights that modulate input and hidden states, are adjusted via the recurrent networks learning process. That is, the cells learn when to allow data to enter, leave or be deleted through the iterative process of making guesses, backpropagating error, and adjusting weights via gradient descent.</p>
<p>The diagram below illustrates how data flows through a memory cell and is controlled by its gates.</p>
<p><img src="./A Beginner&#39;s Guide to LSTMs and Recurrent Neural Networks _ Skymind_files/gers_lstm.png" alt="Alt text"></p>
<p>There are a lot of moving parts here, so if you are new to LSTMs, don’t rush this diagram – contemplate it. After a few minutes, it will begin to reveal its secrets.</p>
<p>Starting from the bottom, the triple arrows show where information flows into the cell at multiple points. That combination of present input and past cell state is fed not only to the cell itself, but also to each of its three gates, which will decide how the input will be handled.</p>
<p>The black dots are the gates themselves, which determine respectively whether to let new input in, erase the present cell state, and/or let that state impact the network’s output at the present time step. <code class="highlighter-rouge">S_c</code> is the current state of the memory cell, and <code class="highlighter-rouge">g_y_in</code> is the current input to it. Remember that each gate can be open or shut, and they will recombine their open and shut states at each step. The cell can forget its state, or not; be written to, or not; and be read from, or not, at each time step, and those flows are represented here.</p>
<p>The large bold letters give us the result of each operation.</p>
<p>Here’s another diagram for good measure, comparing a simple recurrent network (left) to an LSTM cell (right). The blue lines can be ignored; the legend is helpful.</p>
<p><img src="./A Beginner&#39;s Guide to LSTMs and Recurrent Neural Networks _ Skymind_files/greff_lstm_diagram.png" alt="Alt text"></p>
<p>It’s important to note that LSTMs’ memory cells give different roles to addition and multiplication in the transformation of input. The central <strong>plus sign</strong> in both diagrams is essentially the secret of LSTMs. Stupidly simple as it may seem, this basic change helps them preserve a constant error when it must be backpropagated at depth. Instead of determining the subsequent cell state by multiplying its current state with new input, they add the two, and that quite literally makes the difference. (The forget gate still relies on multiplication, of course.)</p>
<p>Different sets of weights filter the input for input, output and forgetting. The forget gate is represented as a linear identity function, because if the gate is open, the current state of the memory cell is simply multiplied by one, to propagate forward one more time step.</p>
<p>Furthermore, while we’re on the topic of simple hacks, <a href="http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf">including a bias of 1</a> to the forget gate of every LSTM cell is also shown to <a href="http://www.felixgers.de/papers/phd.pdf">improve performance</a>. (<em>Sutskever, on the other hand, recommends a bias of 5.</em>)</p>
<p>You may wonder why LSTMs have a forget gate when their purpose is to link distant occurrences to a final output. Well, sometimes it’s good to forget. If you’re analyzing a text corpus and come to the end of a document, for example, you may have no reason to believe that the next document has any relationship to it whatsoever, and therefore the memory cell should be set to zero before the net ingests the first element of the next document.</p>
<p>In the diagram below, you can see the gates at work, with straight lines representing closed gates, and blank circles representing open ones. The lines and circles running horizontal down the hidden layer are the forget gates.</p>
<p><img src="./A Beginner&#39;s Guide to LSTMs and Recurrent Neural Networks _ Skymind_files/gates_lstm.png" alt="Alt text"></p>
<p>It should be noted that while feedforward networks map one input to one output, recurrent nets can map one to many, as above (one image to many words in a caption), many to many (translation), or many to one (classifying a voice).</p>
<h2 id="capturing-diverse-time-scales-and-remote-dependencies"><a name="time">Capturing Diverse Time Scales and Remote Dependencies</a></h2>
<p>You may also wonder what the precise value is of input gates that protect a memory cell from new data coming in, and output gates that prevent it from affecting certain outputs of the RNN. You can think of LSTMs as allowing a neural network to operate on different scales of time at once.</p>
<p>Let’s take a human life, and imagine that we are receiving various streams of data about that life in a time series. Geolocation at each time step is pretty important for the next time step, so that scale of time is always open to the latest information.</p>
<p>Perhaps this human is a diligent citizen who votes every couple years. On democratic time, we would want to pay special attention to what they do around elections, before they return to making a living, and away from larger issues. We would not want to let the constant noise of geolocation affect our political analysis.</p>
<p>If this human is also a diligent daughter, then maybe we can construct a familial time that learns patterns in phone calls which take place regularly every Sunday and spike annually around the holidays. Little to do with political cycles or geolocation.</p>
<p>Other data is like that. Music is polyrhythmic. Text contains recurrent themes at varying intervals. Stock markets and economies experience jitters within longer waves. They operate simultaneously on different time scales that LSTMs can capture.</p>
<h3 id="gated-recurrent-units-grus">Gated Recurrent Units (GRUs)</h3>
<p>A gated recurrent unit (GRU) is basically an LSTM without an output gate, which therefore fully writes the contents from its memory cell to the larger net at each time step.</p>
<p><img src="./A Beginner&#39;s Guide to LSTMs and Recurrent Neural Networks _ Skymind_files/lstm_gru.png" alt="Alt text"></p>
<h2 id="code-sample"><a name="code">Code Sample</a></h2>
<p>A <a href="https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/recurrent/character/LSTMCharModellingExample.java">commented example of a LSTM</a> learning how to replicate Shakespearian drama, and implemented with Deeplearning4j, can be found here. The API is commented where it’s not self-explanatory. If you have questions, please join us on <a href="https://gitter.im/deeplearning4j/deeplearning4j">Gitter</a>.</p>
<p>Here’s what the LSTM configuration looks like:</p>
<script src="./A Beginner&#39;s Guide to LSTMs and Recurrent Neural Networks _ Skymind_files/GravesLSTMCharModellingExample.java"></script>
<h2 id="lstm-hyperparameter-tuning"><a name="tuning">LSTM Hyperparameter Tuning</a></h2>
<p>Here are a few ideas to keep in mind when manually optimizing hyperparameters for RNNs:</p>
<ul>
<li>Watch out for <em>overfitting</em>, which happens when a neural network essentially “memorizes” the training data. Overfitting means you get great performance on training data, but the network’s model is useless for out-of-sample prediction.</li>
<li>Regularization helps: regularization methods include l1, l2, and dropout among others.</li>
<li>So have a separate test set on which the network doesn’t train.</li>
<li>The larger the network, the more powerful, but it’s also easier to overfit. Don’t want to try to learn a million parameters from 10,000 examples – <code class="highlighter-rouge">parameters &gt; examples = trouble</code>.</li>
<li>More data is almost always better, because it helps fight overfitting.</li>
<li>Train over multiple epochs (complete passes through the dataset).</li>
<li>Evaluate test set performance at each epoch to know when to stop (early stopping).</li>
<li>The learning rate is the single most important hyperparameter. Tune this using <a href="http://deeplearning4j.org/visualization">deeplearning4j-ui</a>; see <a href="http://cs231n.github.io/neural-networks-3/#baby">this graph</a></li>
<li>In general, stacking layers can help.</li>
<li>For LSTMs, use the softsign (not softmax) activation function over tanh (it’s faster and less prone to saturation (~0 gradients)).</li>
<li>Updaters: RMSProp, AdaGrad or momentum (Nesterovs) are usually good choices. AdaGrad also decays the learning rate, which can help sometimes.</li>
<li>Finally, remember data normalization, MSE loss function + identity activation function for regression, <a href="https://skymind.ai/wiki/glossary#xavier">Xavier weight initialization</a></li>
</ul>
<h2 id="resources"><a name="resources">Resources</a></h2>
<ul>
<li><a href="http://arxiv.org/pdf/1502.04623v2.pdf">DRAW: A Recurrent Neural Network For Image Generation</a>; (attention models)</li>
<li><a href="http://arxiv.org/pdf/1502.02367v4.pdf">Gated Feedback Recurrent Neural Networks</a></li>
<li><a href="http://people.idsia.ch/~juergen/rnn.html">Recurrent Neural Networks</a>; Juergen Schmidhuber</li>
<li><a href="https://class.coursera.org/neuralnets-2012-001/lecture/77">Modeling Sequences With RNNs and LSTMs</a>; Geoff Hinton</li>
<li><a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a>; Andrej Karpathy</li>
<li><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTMs</a>; Christopher Olah</li>
<li><a href="https://www.cs.cmu.edu/~bhiksha/courses/deeplearning/Fall.2015/pdfs/Werbos.backprop.pdf">Backpropagation Through Time: What It Does and How to Do It</a>; Paul Werbos</li>
<li><a href="http://arxiv.org/pdf/1412.3555v1.pdf">Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling</a>; Cho et al</li>
<li><a href="https://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf">Training Recurrent Neural Networks</a>; Ilya Sutskever’s Dissertation</li>
<li><a href="http://www.cs.toronto.edu/~graves/phd.pdf">Supervised Sequence Labelling with Recurrent Neural Networks</a>; Alex Graves</li>
<li><a href="http://www.felixgers.de/papers/phd.pdf">Long Short-Term Memory in Recurrent Neural Networks</a>; Felix Gers</li>
<li><a href="http://arxiv.org/pdf/1503.04069.pdf">LSTM: A Search Space Oddyssey</a>; Klaus Greff et al</li>
</ul>
<h3 id="footnotes">Footnotes</h3>
<p><a name="one">1)</a> <em>While recurrent networks may seem like a far cry from general artificial intelligence, it’s our belief that intelligence, in fact, is probably dumber than we thought. That is, with a simple feedback loop to serve as memory, we have one of the basic ingredients of consciousness – a necessary but insufficient component. Others, not discussed above, might include additional variables that represent the network and its state, and a framework for decisionmaking logic based on interpretations of data. The latter, ideally, would be part of a larger problem-solving loop that rewards success and punishes failure, much like reinforcement learning. Come to think of it, <a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">DeepMind already built that</a>…</em></p>
<p><a name="two">2)</a> <em>All neural networks whose parameters have been optimized have memory in a sense, because those parameters are the traces of past data. But in feedforward networks, that memory may be frozen in time. That is, after a network is trained, the model it learns may be applied to more data without further adapting itself. In addition, it is monolithic in the sense that the same memory (or set of weights) is applied to all incoming data. Recurrent networks, which also go by the name of dynamic (translation: “changing”) neural networks, are distinguished from feedforward nets not so much by having memory as by giving particular weight to events that occur in a series. While those events do not need to follow each other immediately, they are presumed to be linked, however remotely, by the same temporal thread. Feedforward nets do not make such a presumption. They treat the world as a bucket of objects without order or time. It may be helpful to map two types of neural network to two types of human knowledge. When we are children, we learn to recognize colors, and we go through the rest of our lives recognizing colors wherever we see them, in highly varied contexts and independent of time. We only had to learn the colors once. That knowledge is like memory in feedforward nets; they rely on a past without scope, undefined. Ask them what colors they were fed five minutes ago and they don’t know or care. They are short-term amnesiacs. On the other hand, we also learn as children to decipher the flow of sound called language, and the meanings we extract from sounds such as “toe” or “roe” or “z” are always highly dependent on the sounds preceding (and following) them. Each step of the sequence builds on what went before, and meaning emerges from their order. Indeed, whole sentences conspire to convey the meaning of each syllable within them, their redundant signals acting as a protection against ambient noise. That is similar to the memory of recurrent nets, which look to a particular slice of the past for help. Both types of nets bring the past, or different pasts, to bear in different ways.</em></p>
</div>
</div>
</div>
</div>
<div class="w-section section-cta">
<div class="w-container container-cta">
<div class="w-row"><img width="42" src="./A Beginner&#39;s Guide to LSTMs and Recurrent Neural Networks _ Skymind_files/hr_gradient.png" class="hr-center">
<h1 class="body-h2-cta">Free Consultation</h1>
<p class="body-h3-cta">Schedule a 30-minute Q&amp;A with our AI experts.</p>
<a href="https://skymind.ai/contact" class="w-button button-contactcta">TALK TO A SKYMIND EXPERT</a>
</div>
</div>
</div>
<div class="w-section section-footer">
<div id="footer">
<div class="w-container container-footer"><img src="./A Beginner&#39;s Guide to LSTMs and Recurrent Neural Networks _ Skymind_files/logo_footer.svg" class="img-logo--footer">
<div class="w-row row-2">
<div class="w-col w-col-3">
<h1 class="footer-heading">Company</h1>
<ul class="w-list-unstyled ul-footer">
<li><a class="footer-link" href="https://skymind.ai/about">About</a>
</li>
<li><a class="footer-link" href="https://drive.google.com/drive/folders/1S9n-mdI17euhJMDKP-x0T05E1lGY1ly3" target="_blank">Press Kit</a>
</li>
<li><a class="footer-link" href="https://skymind.ai/contact">Contact Us</a>
</li>
<li><a class="footer-link" href="https://skymind.ai/press">Press</a>
</li>
<li><a class="footer-link" href="https://skymind.ai/privacy">Privacy</a>
</li>
</ul>
</div>
<div class="w-col w-col-2">
<h1 class="footer-heading">What We Offer</h1>
<ul class="w-list-unstyled ul-footer">
<li><a class="footer-link" href="https://skymind.ai/platform">AI Platform</a>
</li>
<li><a class="footer-link" href="https://skymind.ai/subscription">Subscriptions</a>
</li>
</ul>
</div>
<div class="w-col w-col-2">
<h1 class="footer-heading">Open-Source</h1>
<ul class="w-list-unstyled ul-footer">
<li><a class="footer-link" target="_blank" href="http://deeplearning4j.org/">Deeplearning4j</a>
</li>
<li><a class="footer-link" target="_blank" href="http://nd4j.org/">ND4J</a>
</li>
<li><a class="footer-link" target="_blank" href="http://deeplearning4j.org/datavec">DataVec</a>
</li>
<li><a class="footer-link" target="_blank" href="https://github.com/bytedeco/javacpp">JavaCPP</a>
</li>
</ul>
</div>
<div class="w-col w-col-2">
<h1 class="footer-heading">Follow Us</h1>
<ul class="w-list-unstyled ul-footer">
<li><a class="footer-link" target="_blank" href="https://www.facebook.com/deeplearning4j/">Facebook</a>
</li>
<li><a class="footer-link" target="_blank" href="https://twitter.com/deeplearning4j">Twitter</a>
</li>
<li><a class="footer-link" target="_blank" href="https://www.linkedin.com/company/skymind-io">Linkedin</a>
</li>
<li><a class="footer-link" target="_blank" href="https://gitter.im/deeplearning4j/deeplearning4j">Gitter</a>
</li>
</ul>
</div>
<div class="w-col w-col-3 subscribe-form">
<h1 class="footer-heading">Subscribe to our mailing list</h1>
<iframe src="./A Beginner&#39;s Guide to LSTMs and Recurrent Neural Networks _ Skymind_files/d9xsrj.html" width="100%" height="130px" type="text/html" frameborder="0" allowtransparency="true"></iframe>
</div>
</div>
</div>
</div>
</div>

<script>
$(document).ready(function() {
// Tooltip only Text
$('.masterTooltip').hover(function(){
        // Hover over code
        var title = $(this).attr('title');
        $(this).data('tipText', title).removeAttr('title');
        $('<p class="tooltip"></p>')
        .text(title)
        .appendTo('body')
        .fadeIn('slow');
}, function() {
        // Hover out code
        $(this).attr('title', $(this).data('tipText'));
        $('.tooltip').remove();
}).mousemove(function(e) {
        var mousex = e.pageX + 20; //Get X coordinates
        var mousey = e.pageY + 10; //Get Y coordinates
        $('.tooltip')
        .css({ top: mousey, left: mousex })
});
});
</script>
<script type="text/javascript" src="./A Beginner&#39;s Guide to LSTMs and Recurrent Neural Networks _ Skymind_files/jquery.min.js.download"></script>
<script type="text/javascript" src="./A Beginner&#39;s Guide to LSTMs and Recurrent Neural Networks _ Skymind_files/skymind.js.download"></script>
<!--[if lte IE 9]><script src="https://cdnjs.cloudflare.com/ajax/libs/placeholders/3.0.2/placeholders.min.js"></script><![endif]-->
<script type="text/javascript">
    function wikiSearchFn() {
      // Declare variables
      var input, filter, ul, li, a, i;
      input = document.getElementById('wiki-search-input');
      filter = input.value.toUpperCase();
      ul = document.getElementById("wiki-links-ul");
      li = ul.getElementsByTagName('li');

      // Loop through all list items, and hide those who don't match the search query
      for (i = 0; i < li.length; i++) {
          a = li[i].getElementsByTagName("a")[0];
          if (a.innerHTML.toUpperCase().indexOf(filter) > -1) {
              li[i].style.display = "";
          } else {
              li[i].style.display = "none";
          }
      }
    }
  </script>


<script type="text/javascript">( function(){ window.SIG_EXT = {}; } )()</script><script type="text/javascript" id="" src="./A Beginner&#39;s Guide to LSTMs and Recurrent Neural Networks _ Skymind_files/pa-5a1c929041f2c2000700004d.js.download"></script><script type="text/javascript" id="">piAId="457082";piCId="67370";piHostname="pi.pardot.com";(function(){function a(){var b=document.createElement("script");b.type="text/javascript";b.src=("https:"==document.location.protocol?"https://pi":"http://cdn")+".pardot.com/pd.js";var a=document.getElementsByTagName("script")[0];a.parentNode.insertBefore(b,a)}window.attachEvent?window.attachEvent("onload",a):window.addEventListener("load",a,!1)})();</script>
<script type="text/javascript" id="hs-script-loader" src="./A Beginner&#39;s Guide to LSTMs and Recurrent Neural Networks _ Skymind_files/2179705.js(1).download"></script>
<style id="wistia_16_style" type="text/css" class="wistia_injected_style">
@font-face {
font-family: 'WistiaPlayerOverpassNumbers';
src: url(data:application/x-font-ttf;charset=utf-8;base64,AAEAAAARAQAABAAQRFNJRwAAAAEAAA7oAAAACEdQT1Ow+b/jAAAONAAAAKhHU1VCAAEAAAAADtwAAAAKT1MvMl1sVb8AAAe0AAAAYGNtYXAApwIpAAAIFAAAALJjdnQgAAAAAAAAClQAAAAEZnBnbUM+8IgAAAjIAAABCWdhc3AAGgAjAAAOJAAAABBnbHlmWNZE7QAAARwAAAXMaGVhZIS0XikAAAckAAAANmhoZWEF5gGwAAAHkAAAACRobXR4GNICwAAAB1wAAAA0bG9jYQi0CoYAAAcIAAAAHG1heHAAGQBKAAAG6AAAACBuYW1lGpIbcAAAClgAAAOPcG9zdAAPAKQAAA3oAAAAPHByZXBoUamTAAAJ1AAAAH8ACgBd/wYBmgLuAAMADwAVABkAIwApADUAOQA9AEgAAAUhESEHFTMVIxUzNSM1MzUHFTM1IzUHIzUzBxUzFSMVMzUzNQcVIxUzNQcVMzUzFSM1IxUzNQcVMzUHIzUzBxUzBxUzNSM3MzUBmv7DAT3yQUKmQkKmpkIiISFCQkJkQiGFpmQiIWQhpqamIWRkhUZGpmZGIPoD6EMhJSEhJSGBaCJGRiRhISUhRiE8QiJkejgXL1Bxca1xcVAvZyEvISEvIQAAAAIARv/0AiYCyAAVACUAAAQ3Njc2NTQmJyYjIgcGBwYVFBYXFjMmJyY1NDc2MzIXFhUUBwYjAY87MRgTGRo/flo7LxkTGRs9f1wqIR8pX1oqIR4pXgw9M1tJVkOAMnU9MV1IV0Z/MXQ/X0qCeUxmX0uBfEplAAAAAAEAKAAAAOUCvAAIAAATIwYGIxUzETPlLRBHOXdGArwwJyj9wwAAAAABAEcAAAISAsgAJAAAJSE2Nz4CNzY2NzY1NCYjIgcGBxc2MzIWFRQHBgcHBgYHBhUhAhL+fwszEjIhCDBDG0J0Z1c+OhE+HX9HUTMjUhMrOhhEActDPTARJRYFHjAcRFRbaisoQRxxSzs8NSM2DR0uHFJzAAEAMv/0AggCyAA0AAAENjc2NjU0Jic2NjU0JicmJiMiBwYHFzY3NjMyFhcWFRQGIyMVMzIWFRQHBiMiJicHFhcWMwFJViIiJT83Ki8fHBxMKlM7MRpBFR8rPBkvEidLPyUvS1EwLEg+TxpBGzM6YAwfGxxLK0RiFhdSMCdDGBcaLiZAGS4aJBEQIjk6RUBMQkIlIjxCG0spMAAAAAIAHgAAAiICvAAKAA0AACUzNSMRIwEVIRUzAxEjAbhqair+kAFURkb5vTwBw/4mJb0CQ/62AAAAAQBG//QCLgK8AC0AADYWFxYzMjY3NjY1NCYnJiYjIgYHNyE1IQMXNjc2MzIXFhYVFAYHBgYjIicmJwdTLh1ETjpfIyAiIx8fUy4tVCAoASz+nDk7FykzN0QuFBccGBlEJkIuKiQpPB8MHSkjIVUtMVMfHSEeHfQ//pUSGxIWMRc+IiE+GBgbFxUkMwACADz/9AIEAsgAIQA2AAAENjc2NjU0JicmJiMiBgc2Njc2Njc1BgYHBgYVFBYXFhYzEhcWFRQGBwYjIiYnJiY1NDY3NjYzAVFSHx8jIBwdTCo2UxoIMiUlWzFKhDExNh4dHlc4RS0rFxUsSCE7FRYZGBUVOyMMJB8gVTAnTh4fJCEfLFkoKDsPNxJaPz+RSjpjIyYpAYAtLUgiOhUuGBYVOyEjPBYVGAABACgAAAHLArwADAAANjc2NzUhFSEGBwYHM+ooN4L+XQFTdzMrAkamjsSWLjyXqIq3AAAAAwBG//QCEALIACMALwBCAAAABgcGBhUUFhcGBwYVFBYXFjMyNjc2NjU0Jic2NjU0JicmJiMCJjU0NjMyFhUUBiMCJyY1NDY3NjYzMhcWFhUUBwYjAQJJGxoeMCw1JCMiH0JiMFUfHyJEOS4vHhobSSk5RUc3N0dFOUQrLRYVFToiRC4UFi0rRALIHRkZQiQuThQTNTRCLE0cPCAcHE0sQmcVE04vJEIZGR3+0D8zOkVFOjM//pspK0gfOBYWGC4WOB9IKykAAAACADz/9AIEAsgAIAA0AAASBgcGBhUUFhcWFjMyNjcGBgcGBgcVNjY3NjY1NCYnJiMCJyY1NDc2MzIWFxYWFRQGBwYGI/RUICAkIBwbTCo3VRoGLCMkWDJKfy8uMhwbPG1NLSssLUchOxYWGBgVFTsjAsgjIB9WMClNHh4iIyEtXCgpPA83Elo/PpJKOWMlTv58Ly1IRC4vGRYWOyEjPBYWGQAAAAIAMv/yALAB4wALABcAABI2NTQmIyIGFRQWMxI2NTQmIyIGFRQWM4slJRoaJSUaGiUlGholJRoBZSYZGSYmGRkm/o0mGRkmJhkZJgABAAAADQBJAAoAAAAAAAEAAAAAAAEAAAAAAAAAAAAAAAAAYgBiAJ4AsgDsAToBVgGcAfACCgJuAsAC5gABAAAAARmZfAtXkV8PPPUAAwPoAAAAAE2yzjUAAAAA1Z4zgwAe/wYCLgLuAAAABwACAAAAAAAAAfQAXQAAAAACbABGAU4AKAJYAEcCTgAyAksAHgJ0AEYCSgA8AfMAKAJWAEYCSgA8AOIAMgABAAADtv8GAAACdAAAACgCLgABAAAAAAAAAAAAAAAAAAAADQADAhYBkAAFAAgCigJYAAAASwKKAlgAAAFeABQBMgAAAAAFAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABERUxWAEAAIAA6Au7/BgEKA7YA+gAAAAEAAAAAAf8CvAAAACAAAgAAAAMAAAADAAAAigABAAAAAAAcAAMAAQAAAIoABgBuAAAACQAyAAEAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIAAwAEAAUABgAHAAgACQAKAAsADAAEACgAAAAGAAQAAQACACAAOv//AAAAIAAw////4f/SAAEAAAAAAAAAALAALEAOBQYHDQYJFA4TCxIIERBDsAEVRrAJQ0ZhZEJDRUJDRUJDRUJDRrAMQ0ZhZLASQ2FpQkNGsBBDRmFksBRDYWlCQ7BAUHmxBkBCsQUHQ7BAUHmxB0BCsxAFBRJDsBNDYLAUQ2CwBkNgsAdDYLAgYUJDsBFDUrAHQ7BGUlp5swUFBwdDsEBhQkOwQGFCsRAFQ7ARQ1KwBkOwRlJaebMFBQYGQ7BAYUJDsEBhQrEJBUOwEUNSsBJDsEZSWnmxEhJDsEBhQrEIBUOwEUOwQGFQebIGQAZDYEKzDQ8MCkOwEkOyAQEJQxAUEzpDsAZDsApDEDpDsBRDZbAQQxA6Q7AHQ2WwD0MQOi0AAACxAAAAQrE7AEOwAFB5uP+/QBAAAQAAAwQBAAABAAAEAgIAQ0VCQ2lCQ7AEQ0RDYEJDRUJDsAFDsAJDYWpgQkOwA0NEQ2BCHLEtAEOwAVB5swcFBQBDRUJDsF1QebIJBUBCHLIFCgVDYGlCuP/NswABAABDsAVDRENgQhy4LQAdAAAAAAAAAAASAN4AAQAAAAAAAQAWAAAAAQAAAAAAAgAFABYAAQAAAAAAAwAnABsAAQAAAAAABAAcAEIAAQAAAAAABQAPAF4AAQAAAAAABgAcAG0AAQAAAAAACQAgAIkAAQAAAAAACgA4AKkAAwABBAkAAQA4AOEAAwABBAkAAgAOARkAAwABBAkAAwBOAScAAwABBAkABAA4AXUAAwABBAkABQAeAa0AAwABBAkABgA4AXUAAwABBAkACQBAAcsAAwABBAkACgBwAgsAAwABBAkAEAAsAnsAAwABBAkAEQAKAqdXaXN0aWEtUGxheWVyLU92ZXJwYXNzTGlnaHQxLjEwMDtERUxWO1dpc3RpYS1QbGF5ZXItT3ZlcnBhc3MtTGlnaHRXaXN0aWEtUGxheWVyLU92ZXJwYXNzIExpZ2h0VmVyc2lvbiAxLjAzMTAwV2lzdGlhLVBsYXllci1PdmVycGFzcy1MaWdodERlbHZlIFdpdGhyaW5ndG9uLCBUaG9tYXMgSm9ja2luQ29weXJpZ2h0IChjKSAyMDE0IGJ5IFJlZCBIYXQsIEluYy4gQWxsIHJpZ2h0cyByZXNlcnZlZC4AVwBpAHMAdABpAGEALQBQAGwAYQB5AGUAcgAtAE8AdgBlAHIAcABhAHMAcwAgAEwAaQBnAGgAdABSAGUAZwB1AGwAYQByADEALgAxADAAMAA7AEQARQBMAFYAOwBXAGkAcwB0AGkAYQAtAFAAbABhAHkAZQByAC0ATwB2AGUAcgBwAGEAcwBzAC0ATABpAGcAaAB0AFcAaQBzAHQAaQBhAC0AUABsAGEAeQBlAHIALQBPAHYAZQByAHAAYQBzAHMALQBMAGkAZwBoAHQAVgBlAHIAcwBpAG8AbgAgADEALgAwADMAMQAwADAARABlAGwAdgBlACAAVwBpAHQAaAByAGkAbgBnAHQAbwBuACwAIABUAGgAbwBtAGEAcwAgAEoAbwBjAGsAaQBuAEMAbwBwAHkAcgBpAGcAaAB0ACAAKABjACkAIAAyADAAMQA0ACAAYgB5ACAAUgBlAGQAIABIAGEAdAAsACAASQBuAGMALgAgAEEAbABsACAAcgBpAGcAaAB0AHMAIAByAGUAcwBlAHIAdgBlAGQALgBXAGkAcwB0AGkAYQAtAFAAbABhAHkAZQByAC0ATwB2AGUAcgBwAGEAcwBzAEwAaQBnAGgAdAAAAgAAAAAAAP+FABQAAAAAAAAAAAAAAAAAAAAAAAAAAAANAAAAAwATABQAFQAWABcAGAAZABoAGwAcAB0AAQADAAcACgATAAf//wAPAAEAAAAKAB4ALAABREZMVAAIAAQAAAAA//8AAQAAAAFrZXJuAAgAAAABAAAAAQAEAAIAAAABAAgAAQBmAAQAAAAIABoAIAAmADAAOgBIAFIAYAABAAb/7AABAAb/9gACAAn/9gAL//EAAgAJ//YAC//xAAMABP/7AAn/9gAL//YAAgAJ/+wAC//dAAMABv+6AAj/4gAJACMAAQAJ//YAAgABAAMACgAAAAEAAAAAAAAAAAAAAAAAAQAAAAA=);
}
</style></body></html>